import os
from tools.assessment_utils import AssessmentManager
from tools.clip_vector_store import CLIPVectorStore

class EvaluationAgent:
    def __init__(self, assessment_manager=None):
        self.assessment_manager = assessment_manager or AssessmentManager()

    def analyze(self, knowledge_vectorstore_path, project_vectorstore_path, context_prompt):
        print('[DEBUG] EvaluationAgent.analyze called')
        # Check vectorstore paths
        if not project_vectorstore_path or not os.path.exists(project_vectorstore_path):
            error_msg = f"Project vectorstore not found at '{project_vectorstore_path}'."
            print(f'[ERROR] {error_msg}')
            return error_msg
        if not knowledge_vectorstore_path or not os.path.exists(knowledge_vectorstore_path):
            error_msg = f"Knowledge vectorstore not found at '{knowledge_vectorstore_path}'."
            print(f'[ERROR] {error_msg}')
            return error_msg

        # Load vectorstores
        print('[DEBUG] Loading project vectorstore...')
        project_vectorstore = CLIPVectorStore(project_vectorstore_path)
        print('[DEBUG] Loading knowledge vectorstore...')
        knowledge_vectorstore = CLIPVectorStore(knowledge_vectorstore_path)

        # Extract assessment brief (first text-type doc in knowledge vectorstore)
        brief_doc = None
        for meta in knowledge_vectorstore.metadatas:
            if meta.get('type') == 'text':
                brief_doc = meta.get('source', '')
                break
        if brief_doc and os.path.exists(brief_doc):
            with open(brief_doc, 'r', encoding='utf-8', errors='ignore') as f:
                brief_content = f.read()
        else:
            brief_content = ""
        print('[DEBUG] Extracted assessment brief content')

        # Load criteria from context.txt instead of extracting from brief
        try:
            with open('context.txt', 'r', encoding='utf-8') as f:
                extracted_criteria = f.read()
            print('[DEBUG] Loaded criteria from context.txt')
        except Exception as e:
            print(f'[ERROR] Failed to load criteria from context.txt: {e}')
            extracted_criteria = ''

        # Extract code from project (all .java files)
        code_files = []
        for root, _, files in os.walk(os.path.dirname(project_vectorstore_path)):
            for file in files:
                if file.endswith('.java'):
                    code_files.append(os.path.join(root, file))
        code_content = "\n".join(open(f, encoding='utf-8', errors='ignore').read() for f in code_files)
        print(f'[DEBUG] Extracted code content from {len(code_files)} Java files')
        
        # Extract code from project (all .pdf and docx files)
        report_files = []
        for root, _, files in os.walk(os.path.dirname(project_vectorstore_path)):
            for file in files:
                if file.endswith(('.docx', '.pdf')):
                    code_files.append(os.path.join(root, file))
        report_content = "\n".join(open(f, encoding='utf-8', errors='ignore').read() for f in report_files)
        print(f'[DEBUG] Extracted report content from {len(code_files)} report files')
        # Generate feedback using LLM
        feedback_prompt = (
            f"You are an expert code evaluator.\n\nAssessment Criteria:\n{extracted_criteria}\n\nProject Code (excerpt):\n{code_content}Project Report:\n{report_files}\n\nContext:\n{context_prompt}\n\nPlease provide a comprehensive feedback for the student according to the criteria provided.\nBe concise, constructive, and specific, one paragraph feedback per criteria. Do not add any context that this is an AI response."
        )
        feedback = self.assessment_manager.llm.invoke(feedback_prompt)
        print('[DEBUG] Feedback generated by LLM')
        return feedback